import sys
import os
import streamlit as st
from logging import PlaceHolder
from sparkai.embedding.spark_embedding import Embeddingmodel
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_community.document_loaders.markdown import UnstructuredMarkdownLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.llms.sparkllm import SparkLLM
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableLambda, RunnablePassthrough, RunnableParallel, RunnableBranch
from sparkModel import SparkModel
from sparkai_embedding import SparkAIEmbeddings
from dotenv import load_dotenv, find_dotenv
import re


"""
åœ¨æ£€ç´¢é“¾qa_history_chainä¸­ï¼Œæ•°æ®çš„ä¼ é€’æµç¨‹

å‡è®¾åˆå§‹è¾“å…¥å¦‚ä¸‹
{
    "input": "å—ç“œä¹¦è·Ÿå®ƒæœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ",
    "chat_history": [
        ("human", "è¥¿ç“œä¹¦æ˜¯ä»€ä¹ˆï¼Ÿ"),
        ("ai", "è¥¿ç“œä¹¦æ˜¯æŒ‡å‘¨å¿—åè€å¸ˆçš„ã€Šæœºå™¨å­¦ä¹ ã€‹ä¸€ä¹¦ï¼Œæ˜¯æœºå™¨å­¦ä¹ é¢†åŸŸçš„ç»å…¸å…¥é—¨æ•™æä¹‹ä¸€ã€‚"),
    ]
}

ä¸€ã€ç¬¬ä¸€æ­¥ï¼ŒRunnablePassthrough().assign(context=retrieve_docs)

1ã€retrieve_docs åˆ†æ”¯åˆ¤æ–­ï¼Œç”±äºè¾“å…¥åŒ…å« chat_historyï¼Œæ‰€ä»¥èµ°ç¬¬äºŒä¸ªåˆ†æ”¯ï¼Œ
   æ‰§è¡Œ summarize_question_prompt | self.llm | StrOutputParser() | retriever
   
2ã€å·²çŸ¥ summarize_question_prompt å¦‚ä¸‹
    summarize_question_prompt = ChatPromptTemplate([
        ("system", "è¯·æ ¹æ®èŠå¤©è®°å½•æ€»ç»“ç”¨æˆ·æœ€è¿‘çš„é—®é¢˜ï¼Œå¦‚æœæ²¡æœ‰å¤šä½™çš„èŠå¤©è®°å½•ï¼Œåˆ™è¿”å›ç”¨æˆ·çš„é—®é¢˜ã€‚"),
        ("human", "{input}"),
        ("placeholder", "{chat_history}")
    ])
    å°†è¾“å…¥æ•°æ®æ³¨å…¥åˆ° summarize_question_prompt ä¸­ï¼Œå¾—åˆ°å¦‚ä¸‹
    {
        ç³»ç»Ÿ: è¯·æ ¹æ®èŠå¤©è®°å½•æ€»ç»“ç”¨æˆ·æœ€è¿‘çš„é—®é¢˜ï¼Œå¦‚æœæ²¡æœ‰å¤šä½™çš„èŠå¤©è®°å½•ï¼Œåˆ™è¿”å›ç”¨æˆ·çš„é—®é¢˜ã€‚

        ç”¨æˆ·: å—ç“œä¹¦è·Ÿå®ƒæœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ

        èŠå¤©è®°å½•: 
            ç”¨æˆ·: è¥¿ç“œä¹¦æ˜¯ä»€ä¹ˆï¼Ÿ
            AI: è¥¿ç“œä¹¦æ˜¯æŒ‡å‘¨å¿—åè€å¸ˆçš„ã€Šæœºå™¨å­¦ä¹ ã€‹ä¸€ä¹¦ï¼Œæ˜¯æœºå™¨å­¦ä¹ é¢†åŸŸçš„ç»å…¸å…¥é—¨æ•™æä¹‹ä¸€ã€‚
    }
    
3ã€å°† summarize_question_prompt å‘é€ç»™llmå¤„ç†ï¼Œllmå¯èƒ½ä¼šè¿”å›å¦‚ä¸‹
    "å—ç“œä¹¦ä¸è¥¿ç“œä¹¦çš„å…³ç³»æ˜¯ä»€ä¹ˆï¼Ÿ"
    
4ã€å°†llmçš„è¾“å‡ºé€šè¿‡StrOutputParser()è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œç„¶ååˆ©ç”¨llmå®Œå–„åçš„é—®é¢˜æ£€ç´¢å‘é‡æ•°æ®åº“
    retriever.invoke("å—ç“œä¹¦ä¸è¥¿ç“œä¹¦çš„å…³ç³»æ˜¯ä»€ä¹ˆï¼Ÿ")
    
5ã€å‡è®¾æ£€ç´¢è¿”å›æ–‡æ¡£å¦‚ä¸‹
    [
        Document(page_content="å—ç“œä¹¦æ˜¯ã€Šæœºå™¨å­¦ä¹ ã€‹çš„è¡¥å……æ•™æï¼Œä¸è¥¿ç“œä¹¦å½¢æˆé…å¥—...", metadata={...}),
        Document(page_content="å—ç“œä¹¦è¯¦ç»†è§£é‡Šäº†è¥¿ç“œä¹¦ä¸­çš„æ•°å­¦æ¨å¯¼...", metadata={...})
    ]

6ã€æœ€ç»ˆï¼ŒRunnablePassthrough().assign(context=retrieve_docs)çš„è¾“å‡ºå¦‚ä¸‹
    {
        "input": "å—ç“œä¹¦è·Ÿå®ƒæœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ",
        "chat_history": [
            ("human", "è¥¿ç“œä¹¦æ˜¯ä»€ä¹ˆï¼Ÿ"),
            ("ai", "è¥¿ç“œä¹¦æ˜¯æŒ‡å‘¨å¿—åè€å¸ˆçš„ã€Šæœºå™¨å­¦ä¹ ã€‹ä¸€ä¹¦ï¼Œæ˜¯æœºå™¨å­¦ä¹ é¢†åŸŸçš„ç»å…¸å…¥é—¨æ•™æä¹‹ä¸€ã€‚"),
        ],
        "context": [
            Document(page_content="å—ç“œä¹¦æ˜¯ã€Šæœºå™¨å­¦ä¹ ã€‹çš„è¡¥å……æ•™æï¼Œä¸è¥¿ç“œä¹¦å½¢æˆé…å¥—...", metadata={...}),
            Document(page_content="å—ç“œä¹¦è¯¦ç»†è§£é‡Šäº†è¥¿ç“œä¹¦ä¸­çš„æ•°å­¦æ¨å¯¼...", metadata={...})
        ]
    }

ç¬¬äºŒæ­¥ï¼ŒRunnablePassthrough().assign(answer=qa_chain)

1ã€å·²çŸ¥ qa_chain = (
            RunnablePassthrough().assign(context=self.combine_docs)
            | qa_prompt
            | self.llm 
            | StrOutputParser()
        )

2ã€ç»è¿‡ combine_docs() æ–¹æ³•å¤„ç†ï¼Œå°†æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹åˆå¹¶ä¸ºä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œå¦‚ä¸‹
    "å—ç“œä¹¦æ˜¯ã€Šæœºå™¨å­¦ä¹ ã€‹çš„è¡¥å……æ•™æï¼Œä¸è¥¿ç“œä¹¦å½¢æˆé…å¥—...

     å—ç“œä¹¦è¯¦ç»†è§£é‡Šäº†è¥¿ç“œä¹¦ä¸­çš„æ•°å­¦æ¨å¯¼..."

3ã€å·²çŸ¥ qa_prompt = ChatPromptTemplate([
            ("system", "ä½ æ˜¯ä¸€ä¸ªé—®ç­”ä»»åŠ¡çš„åŠ©æ‰‹ã€‚
                        è¯·ä½¿ç”¨æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ç‰‡æ®µå›ç­”è¿™ä¸ªé—®é¢˜ã€‚
                        å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆå°±è¯´ä¸çŸ¥é“ã€‚è¯·ä½¿ç”¨ç®€æ´çš„è¯è¯­å›ç­”ç”¨æˆ·ã€‚
                        \n\n{context}"),
            ("human", "{input}"),
            ("placeholder", "{chat_history}")
        ])

    å°†åˆå¹¶åçš„æ–‡æ¡£å†…å®¹å’ŒåŸå§‹è¾“å…¥æ•°æ®æ³¨å…¥åˆ° qa_prompt ä¸­ï¼Œå¾—åˆ°å¦‚ä¸‹
    {
        ç³»ç»Ÿ: ä½ æ˜¯ä¸€ä¸ªé—®ç­”ä»»åŠ¡çš„åŠ©æ‰‹ã€‚è¯·ä½¿ç”¨æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ç‰‡æ®µå›ç­”è¿™ä¸ªé—®é¢˜ã€‚å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆå°±è¯´ä¸çŸ¥é“ã€‚è¯·ä½¿ç”¨ç®€æ´çš„è¯è¯­å›ç­”ç”¨æˆ·ã€‚

        å—ç“œä¹¦æ˜¯ã€Šæœºå™¨å­¦ä¹ ã€‹çš„è¡¥å……æ•™æï¼Œä¸è¥¿ç“œä¹¦å½¢æˆé…å¥—...

        å—ç“œä¹¦è¯¦ç»†è§£é‡Šäº†è¥¿ç“œä¹¦ä¸­çš„æ•°å­¦æ¨å¯¼...

        ç”¨æˆ·: å—ç“œä¹¦è·Ÿå®ƒæœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ

        èŠå¤©è®°å½•: 
            ç”¨æˆ·: è¥¿ç“œä¹¦æ˜¯ä»€ä¹ˆï¼Ÿ
            AI: è¥¿ç“œä¹¦æ˜¯æŒ‡å‘¨å¿—åè€å¸ˆçš„ã€Šæœºå™¨å­¦ä¹ ã€‹ä¸€ä¹¦ï¼Œæ˜¯æœºå™¨å­¦ä¹ é¢†åŸŸçš„ç»å…¸å…¥é—¨æ•™æä¹‹ä¸€ã€‚
    }
    
4ã€å°† qa_prompt å‘ç»™ llmï¼ŒllmåŸºäºä¸Šä¸‹æ–‡å’ŒèŠå¤©å†å²ç”Ÿæˆç­”æ¡ˆï¼Œæ¯”å¦‚
    "å—ç“œä¹¦æ˜¯è¥¿ç“œä¹¦çš„è¡¥å……æ•™æï¼Œä¸“é—¨ç”¨äºè¯¦ç»†è§£é‡Šè¥¿ç“œä¹¦ä¸­å¤æ‚çš„æ•°å­¦æ¨å¯¼è¿‡ç¨‹ï¼Œä¸¤è€…å½¢æˆé…å¥—å…³ç³»ã€‚"

5ã€æœ€ç»ˆï¼Œæ£€ç´¢é“¾qa_history_chainçš„è¾“å‡ºå¦‚ä¸‹
    {
        "input": "å—ç“œä¹¦è·Ÿå®ƒæœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ",
        "chat_history": [
            ("human", "è¥¿ç“œä¹¦æ˜¯ä»€ä¹ˆï¼Ÿ"),
            ("ai", "è¥¿ç“œä¹¦æ˜¯æŒ‡å‘¨å¿—åè€å¸ˆçš„ã€Šæœºå™¨å­¦ä¹ ã€‹ä¸€ä¹¦ï¼Œæ˜¯æœºå™¨å­¦ä¹ é¢†åŸŸçš„ç»å…¸å…¥é—¨æ•™æä¹‹ä¸€ã€‚"),
        ],
        "context": [
            Document(page_content="å—ç“œä¹¦æ˜¯ã€Šæœºå™¨å­¦ä¹ ã€‹çš„è¡¥å……æ•™æï¼Œä¸è¥¿ç“œä¹¦å½¢æˆé…å¥—...", metadata={...}),
            Document(page_content="å—ç“œä¹¦è¯¦ç»†è§£é‡Šäº†è¥¿ç“œä¹¦ä¸­çš„æ•°å­¦æ¨å¯¼...", metadata={...})
        ],
        "answer": "å—ç“œä¹¦æ˜¯è¥¿ç“œä¹¦çš„è¡¥å……æ•™æï¼Œä¸“é—¨ç”¨äºè¯¦ç»†è§£é‡Šè¥¿ç“œä¹¦ä¸­å¤æ‚çš„æ•°å­¦æ¨å¯¼è¿‡ç¨‹ï¼Œä¸¤è€…å½¢æˆé…å¥—å…³ç³»ã€‚"
    }
"""


class PersonalAssistant:
    def __init__(self):
        _ = load_dotenv(find_dotenv())
        self.spark_appid=os.environ.get("IFLYTEK_SPARK_APP_ID")
        self.spark_api_key=os.environ.get("IFLYTEK_SPARK_API_KEY")
        self.spark_api_secret=os.environ.get("IFLYTEK_SPARK_API_SECRET")
        self.spark_x1_url=os.environ.get("IFLYTEK_SPARK_X1_URL")
        self.spark_4ultra_url=os.environ.get("IFLYTEK_SPARK_4Ultra_URL")
        self.spark_4ultra_domain=os.environ.get("IFLYTEK_SPARK_4Ultra_DOMAIN")
        # åŸºäºlangchainè°ƒç”¨å¤§æ¨¡å‹
        self.llm = SparkLLM(
            spark_app_id=self.spark_appid,
            spark_api_key=self.spark_api_key,
            spark_api_secret=self.spark_api_secret,
            spark_llm_domain=self.spark_4ultra_domain,
            spark_api_url=self.spark_4ultra_url,
            temperature=0.1
        )
    
    # è·å–ç”±å‘é‡æ•°æ®åº“æ„å»ºçš„æ£€ç´¢å™¨
    def get_retriever(self):
        # æ˜Ÿç«æ–‡æœ¬å‘é‡åŒ–
        self.sparkEmbedding = SparkAIEmbeddings()
        self.database_directory = 'data_base/vector_db/chroma'
        vector_db = Chroma(
            embedding_function=self.sparkEmbedding,
            persist_directory=self.database_directory
        )
        # é€šè¿‡as_retrieveræ–¹æ³•æŠŠå‘é‡æ•°æ®åº“æ„é€ æˆæ£€ç´¢å™¨
        retriever = vector_db.as_retriever()
        return retriever
    
    # å¤„ç†æ£€ç´¢å™¨è¿”å›çš„æ–‡æœ¬
    def combine_docs(self, docs):
        return "\n\n".join(doc.page_content for doc in docs["context"])
    
    # æ„å»ºä¸€ä¸ªå¸¦å†å²èŠå¤©è®°å½•çš„æ£€ç´¢é—®ç­”é“¾
    def get_qa_history_chain(self):
        retriever = self.get_retriever()
        summarize_question_system_template = (
            "è¯·æ ¹æ®èŠå¤©è®°å½•æ€»ç»“ç”¨æˆ·æœ€è¿‘çš„é—®é¢˜ï¼Œ"
            "å¦‚æœæ²¡æœ‰å¤šä½™çš„èŠå¤©è®°å½•ï¼Œåˆ™è¿”å›ç”¨æˆ·çš„é—®é¢˜ã€‚"
        )
        summarize_question_prompt = ChatPromptTemplate([
            ("system", summarize_question_system_template),
            ("human", "{input}"),
            ("placeholder", "{chat_history}")
        ])
        # åˆ©ç”¨llmå®Œå–„ç”¨æˆ·é—®é¢˜ï¼Œç”¨æ¥æ£€ç´¢å‘é‡æ•°æ®åº“
        retrieve_docs = RunnableBranch(
            (lambda x : not x.get("chat_history", False), (lambda x : x.get("input") | retriever)),
            summarize_question_prompt | self.llm | StrOutputParser() | retriever
        )
        
        system_prompt = (
            "ä½ æ˜¯ä¸€ä¸ªé—®ç­”ä»»åŠ¡çš„åŠ©æ‰‹ã€‚ "
            "è¯·ä½¿ç”¨æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ç‰‡æ®µå›ç­”è¿™ä¸ªé—®é¢˜ã€‚ "
            "å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆå°±è¯´ä¸çŸ¥é“ã€‚ "
            "è¯·ä½¿ç”¨ç®€æ´çš„è¯è¯­å›ç­”ç”¨æˆ·ã€‚"
            "\n\n"
            "{context}"
        )
        qa_prompt = ChatPromptTemplate([
            ("system", system_prompt),
            ("human", "{input}"),
            ("placeholder", "{chat_history}")
        ])
        qa_chain = (
            RunnablePassthrough().assign(context=self.combine_docs)
            | qa_prompt
            | self.llm 
            | StrOutputParser()
        )
        # RunnablePassthrough().assign() ä½œç”¨ï¼šä¿æŒæ‰€æœ‰åŸå§‹è¾“å…¥æ•°æ®ä¸å˜ï¼Œ
        #                                      å¹¶ä¸ºæ•°æ®å­—å…¸æ·»åŠ æ–°çš„é”®å€¼å¯¹ï¼ˆä¹Ÿå¯ä»¥è¦†ç›–ç°æœ‰å­—æ®µçš„å€¼ï¼‰
        qa_history_chain = RunnablePassthrough().assign(
            context = retrieve_docs,
        ).assign(
            answer = qa_chain
        )
        return qa_history_chain
    
    # æ¥å—æ£€ç´¢é—®ç­”é“¾ã€ç”¨æˆ·è¾“å…¥åŠèŠå¤©å†å²ï¼Œå¹¶ä»¥æµå¼è¿”å›è¯¥é“¾è¾“å‡º
    def gen_response(self, chain, input, chat_history):
        response = chain.stream({
            "input": input,
            "chat_history": chat_history
        })
        for res in response:
            if "answer" in res.keys():
                yield res["answer"]

def main():
    assistant = PersonalAssistant()
    st.markdown('### ğŸ¦œğŸ”— åŠ¨æ‰‹å­¦å¤§æ¨¡å‹åº”ç”¨å¼€å‘')
    # st.session_stateå¯ä»¥å­˜å‚¨ç”¨æˆ·ä¸åº”ç”¨äº¤äº’æœŸé—´çš„çŠ¶æ€ä¸æ•°æ®
    # å­˜å‚¨å¯¹è¯å†å²
    if "messages" not in st.session_state:
        st.session_state.messages = []
    # å­˜å‚¨æ£€ç´¢é—®ç­”é“¾
    if "qa_history_chain" not in st.session_state:
        st.session_state.qa_history_chain = assistant.get_qa_history_chain()
    # å»ºç«‹å®¹å™¨ é«˜åº¦ä¸º500 px
    messages = st.container(height=550)
    # æ˜¾ç¤ºæ•´ä¸ªå¯¹è¯å†å²
    for message in st.session_state.messages: # éå†å¯¹è¯å†å²
            with messages.chat_message(message[0]): # messagesæŒ‡åœ¨å®¹å™¨ä¸‹æ˜¾ç¤ºï¼Œchat_messageæ˜¾ç¤ºç”¨æˆ·åŠaiå¤´åƒ
                st.write(message[1]) # æ‰“å°å†…å®¹
    if prompt := st.chat_input("Say something"):
        # å°†ç”¨æˆ·è¾“å…¥æ·»åŠ åˆ°å¯¹è¯å†å²ä¸­
        st.session_state.messages.append(("human", prompt))
        # æ˜¾ç¤ºå½“å‰ç”¨æˆ·è¾“å…¥
        with messages.chat_message("human"):
            st.write(prompt)
        # ç”Ÿæˆå›å¤
        answer = assistant.gen_response(
            chain=st.session_state.qa_history_chain,
            input=prompt,
            chat_history=st.session_state.messages
        )
        # æµå¼è¾“å‡º
        with messages.chat_message("ai"):
            output = st.write_stream(answer)
        # å°†è¾“å‡ºå­˜å…¥st.session_state.messages
        st.session_state.messages.append(("ai", output))


if __name__ == "__main__":
    main()
